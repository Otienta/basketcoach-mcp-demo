# basketcoach-mcp/rag/embed.py
#!/usr/bin/env python3
"""
Syst√®me d'embedding et de recherche RAG pour les guidelines basketball
"""

import os
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional
import pickle
import logging
from pathlib import Path

from sentence_transformers import SentenceTransformer
import faiss
import PyPDF2
from sklearn.metrics.pairwise import cosine_similarity
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.logger import get_logger
from utils.config import get_config

logger = get_logger("rag.embed")

class RAGSystem:
    """Syst√®me RAG pour la recherche dans les guidelines basketball"""
    
    def __init__(self, model_name: str = "BAAI/bge-large-en-v1.5"):
        self.config = get_config()
        self.model_name = model_name
        self.model = None
        self.index = None
        self.guidelines_data = []
        self.is_initialized = False
        
        # Chemins
        self.guidelines_path = Path(self.config.get("rag.guidelines_path", "rag/guidelines/"))
        self.embeddings_path = Path("rag/embeddings/")
        self.database_path = Path("rag/database/")
        
        # Cr√©ation des r√©pertoires
        self.embeddings_path.mkdir(parents=True, exist_ok=True)
        self.database_path.mkdir(parents=True, exist_ok=True)
    
    def initialize(self):
        """Initialise le syst√®me RAG"""
        try:
            logger.info("üöÄ Initialisation du syst√®me RAG...")
            
            # Chargement du mod√®le
            self.model = SentenceTransformer(self.model_name)
            logger.info(f"‚úÖ Mod√®le charg√©: {self.model_name}")
            
            # Chargement ou cr√©ation des embeddings
            if self._check_existing_embeddings():
                self._load_existing_embeddings()
            else:
                self._process_guidelines()
                self._create_embeddings()
            
            self.is_initialized = True
            logger.info("‚úÖ Syst√®me RAG initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation RAG: {e}")
            raise
    
    def search(self, query: str, top_k: int = 5, similarity_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """
        Recherche s√©mantique dans les guidelines - seuil r√©duit
        """
        if not self.is_initialized:
            self.initialize()
        
        try:
            # Embedding de la requ√™te
            query_embedding = self.model.encode([query])
            
            # Recherche √©tendue
            distances, indices = self.index.search(query_embedding, top_k * 2)
            
            # R√©cup√©ration des r√©sultats avec seuil r√©duit
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx < len(self.guidelines_data):
                    guideline = self.guidelines_data[idx]
                    # Score de similarit√© normalis√©
                    similarity_score = float(distance)
                    
                    if similarity_score >= similarity_threshold:
                        results.append({
                            "rank": len(results) + 1,
                            "content": guideline["content"],
                            "source": guideline["source"],
                            "category": guideline["category"],
                            "similarity_score": similarity_score,
                            "page": guideline.get("page", "N/A")
                        })
                    
                    # Arr√™ter quand on a assez de r√©sultats
                    if len(results) >= top_k:
                        break
            
            logger.info(f"üîç Recherche '{query}': {len(results)} r√©sultats (seuil: {similarity_threshold})")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche RAG: {e}")
            return []
    
    def add_guideline(self, content: str, source: str, category: str, metadata: Dict = None):
        """
        Ajoute une nouvelle guideline au syst√®me
        """
        if not self.is_initialized:
            self.initialize()
        
        try:
            guideline = {
                "content": content,
                "source": source,
                "category": category,
                "metadata": metadata or {}
            }
            
            # Ajout aux donn√©es
            self.guidelines_data.append(guideline)
            
            # Mise √† jour des embeddings
            self._update_embeddings([guideline])
            
            logger.info(f"‚úÖ Guideline ajout√©e: {source} - {category}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur ajout guideline: {e}")
    
    def _check_existing_embeddings(self) -> bool:
        """V√©rifie si des embeddings existent d√©j√†"""
        index_path = self.embeddings_path / "guidelines.index"
        data_path = self.database_path / "guidelines_data.pkl"
        
        return index_path.exists() and data_path.exists()
    
    def _load_existing_embeddings(self):
        """Charge les embeddings existants"""
        try:
            # Chargement des donn√©es
            data_path = self.database_path / "guidelines_data.pkl"
            with open(data_path, 'rb') as f:
                self.guidelines_data = pickle.load(f)
            
            # Chargement de l'index FAISS
            index_path = self.embeddings_path / "guidelines.index"
            self.index = faiss.read_index(str(index_path))
            
            logger.info(f"‚úÖ Embeddings charg√©s: {len(self.guidelines_data)} guidelines")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement embeddings: {e}")
            raise
    
    def _process_guidelines(self):
        """Traite les fichiers PDF de guidelines ‚Äì VERSION OPTIMIS√âE"""
        logger.info("üìö Traitement des guidelines...")

        # R√©initialisation obligatoire pour √©viter les doublons
        self.guidelines_data = []
        
        # Liste des PDF trouv√©s
        pdf_files = list(self.guidelines_path.glob("*.pdf"))
        logger.info(f"üîç {len(pdf_files)} fichiers PDF trouv√©s dans {self.guidelines_path}")

        # ----- 1) PRIORIT√â AUX PDF -----
        if pdf_files:
            for pdf_file in pdf_files:
                try:
                    logger.info(f"üìÑ Extraction du PDF : {pdf_file.name}")
                    pdf_guidelines = self._extract_text_from_pdf(pdf_file)
                    self.guidelines_data.extend(pdf_guidelines)
                    logger.info(f"   ‚Üí {len(pdf_guidelines)} extraits ajout√©s")
                except Exception as e:
                    logger.error(f"‚ùå Erreur traitement PDF {pdf_file}: {e}")
        else:
            logger.warning("‚ö†Ô∏è Aucun PDF trouv√© ‚Äì recours aux guidelines par d√©faut")

            # ----- 2) GUIDELINES PAR D√âFAUT -----
            self.guidelines_data = [
                {
                    "content": "ESC 2024: Limiter les s√©ances intensives √† 2 par semaine maximum pour pr√©venir le surentra√Ænement",
                    "source": "European Society of Cardiology 2024",
                    "category": "entra√Ænement",
                    "page": "12"
                },
                {
                    "content": "Recommandation EU: 48h de repos entre deux matches comp√©titifs pour une r√©cup√©ration optimale",
                    "source": "European Basketball Union 2023",
                    "category": "r√©cup√©ration",
                    "page": "8"
                },
                {
                    "content": "Protocole hydratation: 500ml 2h avant l'effort, 250ml toutes les 20min pendant l'activit√©",
                    "source": "International Journal of Sports Medicine",
                    "category": "nutrition",
                    "page": "15"
                },
                {
                    "content": "Cheville: Protocole RICE (Repos, Ice, Compression, √âl√©vation) 48h pour entorses l√©g√®res",
                    "source": "Journal of Orthopaedic Surgery 2024",
                    "category": "blessure",
                    "page": "22"
                },
                {
                    "content": "Genou: Consultation imm√©diate recommand√©e si gonflement > 2cm apr√®s traumatisme",
                    "source": "American Journal of Sports Medicine",
                    "category": "blessure",
                    "page": "18"
                },
                {
                    "content": "Apport prot√©ique: 1.6-2.2g/kg/jour recommand√© pour les sportives d'√©lite en basketball",
                    "source": "International Society of Sports Nutrition",
                    "category": "nutrition",
                    "page": "7"
                },
                {
                    "content": "Sommeil: 8-10h/nuit requis pour les sportives professionnelles pour une r√©cup√©ration optimale",
                    "source": "Sleep Medicine Journal",
                    "category": "r√©cup√©ration",
                    "page": "11"
                },
                {
                    "content": "Pr√©vention blessures: Programme de renforcement musculaire 3x/semaine r√©duit les risques de 40%",
                    "source": "British Journal of Sports Medicine",
                    "category": "pr√©vention",
                    "page": "9"
                }
            ]

        # ----- 3) V√©rification finale -----
        if not self.guidelines_data:
            logger.error("‚ùå Aucune guideline disponible !")
            raise Exception("Aucune donn√©e guideline trouv√©e")

        logger.info(f"üìä Total guidelines charg√©es : {len(self.guidelines_data)}")

        # ----- 4) Sauvegarde -----
        with open(self.database_path / "guidelines_data.pkl", 'wb') as f:
            pickle.dump(self.guidelines_data, f)

    
    def _extract_text_from_pdf(self, pdf_path: Path) -> List[Dict[str, Any]]:
        """Extrait le texte d'un fichier PDF"""
        guidelines = []
        
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            for page_num, page in enumerate(pdf_reader.pages, 1):
                text = page.extract_text()
                if text.strip():
                    # Segmentation en chunks (simplifi√©)
                    chunks = self._split_text_into_chunks(text)
                    
                    for chunk in chunks:
                        guidelines.append({
                            "content": chunk,
                            "source": pdf_path.name,
                            "category": "g√©n√©ral",
                            "page": str(page_num)
                        })
        
        return guidelines
    
    def _split_text_into_chunks(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """Segmente le texte en chunks pour l'embedding"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
            
            if i + chunk_size >= len(words):
                break
        
        return chunks
    
    def _create_embeddings(self):
        """Cr√©e les embeddings pour toutes les guidelines"""
        logger.info("üî® Cr√©ation des embeddings...")
        
        try:
            # Extraction du contenu
            contents = [guideline["content"] for guideline in self.guidelines_data]
            
            # Cr√©ation des embeddings
            embeddings = self.model.encode(contents, show_progress_bar=True)
            
            # Cr√©ation de l'index FAISS
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # Produit scalaire pour similarit√© cosinus
            
            # Normalisation pour similarit√© cosinus
            faiss.normalize_L2(embeddings)
            self.index.add(embeddings)
            
            # Sauvegarde de l'index
            faiss.write_index(self.index, str(self.embeddings_path / "guidelines.index"))
            
            logger.info(f"‚úÖ Embeddings cr√©√©s: {len(self.guidelines_data)} guidelines")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation embeddings: {e}")
            raise
    
    def _update_embeddings(self, new_guidelines: List[Dict]):
        """Met √† jour les embeddings avec de nouvelles guidelines"""
        try:
            # Embeddings des nouvelles guidelines
            new_contents = [guideline["content"] for guideline in new_guidelines]
            new_embeddings = self.model.encode(new_contents)
            
            # Ajout √† l'index existant
            faiss.normalize_L2(new_embeddings)
            self.index.add(new_embeddings)
            
            # Sauvegarde de l'index mis √† jour
            faiss.write_index(self.index, str(self.embeddings_path / "guidelines.index"))
            
            # Sauvegarde des donn√©es mises √† jour
            with open(self.database_path / "guidelines_data.pkl", 'wb') as f:
                pickle.dump(self.guidelines_data, f)
            
            logger.info(f"‚úÖ Embeddings mis √† jour: {len(new_guidelines)} nouvelles guidelines")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mise √† jour embeddings: {e}")

# Instance globale
rag_system = RAGSystem()

def initialize_rag():
    """Initialise le syst√®me RAG au d√©marrage"""
    rag_system.initialize()

def search_guidelines(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """Fonction de recherche principale pour le serveur MCP"""
    return rag_system.search(query, top_k)